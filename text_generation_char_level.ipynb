{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_generation_char_level.ipynb",
      "provenance": [],
      "mount_file_id": "1kNonsPgfQOfp8d51GOakRC-ev_SUbsnD",
      "authorship_tag": "ABX9TyN0da4NyMBMF9ZG4pf0umS/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanuja1521/Text-generation/blob/master/text_generation_char_level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fOvIEg8K2YH",
        "colab_type": "text"
      },
      "source": [
        "#Importing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNiFzkR7Km9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOH2exvNkxLg",
        "colab_type": "text"
      },
      "source": [
        "#Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBenuYAVmZDS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "134048db-4ebd-4914-fef2-03586574311d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiF-j4h5mhmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = '/content/gdrive/My Drive/Colab Notebooks/text.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RglHHn9qKtIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLxWTM7KQYJo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5e086872-8f50-410f-e155-bfb827739f6d"
      },
      "source": [
        "print ('Length of text: {} characters'.format(len(text)))\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 17134 characters\n",
            "92 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGFy4dz7QneG",
        "colab_type": "text"
      },
      "source": [
        "# PreProcessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FBdv_bzITMF",
        "colab_type": "text"
      },
      "source": [
        "**Vectorizing the text** \n",
        "\n",
        "Creating two mappings: characters to numbers and numbers to characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-qYszmQQxbP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b8c1ad2-7edc-4fd4-d1d2-fcf271d1978b"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "text_as_int[:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([37, 81, 61, 70,  2, 68, 79, 24, 39, 76])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1AWrgDMTeCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68d32cbd-7395-4fbc-b211-21e6072965c7"
      },
      "source": [
        "idx2char[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['\\n', '\\r', ' ', '!', '(', ')', ',', '-', '.', '0'], dtype='<U1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqUoB-rJTzvM",
        "colab_type": "text"
      },
      "source": [
        "# Creating Training and Target Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXoy60JpIrPC",
        "colab_type": "text"
      },
      "source": [
        "**Dividing the text into example sequences.**\n",
        "\n",
        " Each input sequence will contain `seq_length` characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.So break the text into chunks of `seq_length+1`. \n",
        "For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "`tf.data.Dataset.from_tensor_slices` function converts the text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib84pHakT6LV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35f7ea98-d3a6-4656-f65f-cb2a640bcb0c"
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "char_dataset"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: (), types: tf.int64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeMdzAXPJpjF",
        "colab_type": "text"
      },
      "source": [
        "The `batch` method converts these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD8NTo2bUYzz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a30b331-6008-47e5-a708-097229f912a1"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "sequences"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: (101,), types: tf.int64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8ikR_odJvNf",
        "colab_type": "text"
      },
      "source": [
        "For each sequence, form the input and target text and map them using the map method to each `batch`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwUCMX6yUhok",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7de7bba5-34b4-4bd4-d8b4-bf2bd90b5edb"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "dataset"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLXXTOoUUpzN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b0ee8fd8-c663-4c2e-805c-0f7b87933c6f"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'क्या होंगे आर्टिकल 370 ख़त्म होने के राजनैतिक मायने? जम्मू में किस तरह और कश्मीर में किस तरह इस बदलाव'\n",
            "Target data: '्या होंगे आर्टिकल 370 ख़त्म होने के राजनैतिक मायने? जम्मू में किस तरह और कश्मीर में किस तरह इस बदलाव '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoY9p350UrMX",
        "colab_type": "text"
      },
      "source": [
        "# Creating Training Batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwEEY5OHKTfI",
        "colab_type": "text"
      },
      "source": [
        "Before feeding this data into the model, we need to shuffle the data and pack it into batches of desired size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-DLh_QLUwwz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97396aa6-f49a-4ee8-8e3e-f878eef1fea5"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# TF data is designed to work with possibly infinite sequences, so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements.\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-JtguNQVC80",
        "colab_type": "text"
      },
      "source": [
        "# Building The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L32nXv8KhGQ",
        "colab_type": "text"
      },
      "source": [
        "Using `tf.keras.Sequential` to define the model. \n",
        "\n",
        "For this simple example three layers are used to define our model:\n",
        "\n",
        "*   `tf.keras.layers.Embedding`: The input layer that will map the numbers of each character to a vector with `embedding_dim `dimensions;\n",
        "*   `tf.keras.layers.LSTM`: A type of LSTM with size units=`rnn_units`\n",
        "*   `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQP5wUa_VHPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpY87q9YVV3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_Lj7SYWVeyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mb73dXuLnqW",
        "colab_type": "text"
      },
      "source": [
        "For each character the model looks up the embedding, runs the LSTM one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f00FprzZVj4E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "5529da69-e668-4b92-9517-625f160f244e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           23552     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 92)            94300     \n",
            "=================================================================\n",
            "Total params: 5,364,828\n",
            "Trainable params: 5,364,828\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLsYXM8FVpMD",
        "colab_type": "text"
      },
      "source": [
        "#Training The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id9v01DjbaZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhJ4De0eL3eC",
        "colab_type": "text"
      },
      "source": [
        "Configuring the training procedure using the `tf.keras.Model.compile `method.  `tf.keras.optimizers.Adam` with default arguments and the `loss function` are used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La22SrwGVtV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqlw1GlnMFDp",
        "colab_type": "text"
      },
      "source": [
        "`tf.keras.callbacks.ModelCheckpoint` ensures that checkpoints are saved during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvVIeyLFWOr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubW-XahKWUX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs7CIkuyWXK-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b5df7d31-8d9c-4219-885d-999d38a9af5d"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 4.5022\n",
            "Epoch 2/30\n",
            "2/2 [==============================] - 2s 822ms/step - loss: 3.8370\n",
            "Epoch 3/30\n",
            "2/2 [==============================] - 2s 825ms/step - loss: 3.4676\n",
            "Epoch 4/30\n",
            "2/2 [==============================] - 0s 88ms/step - loss: 3.4202\n",
            "Epoch 5/30\n",
            "2/2 [==============================] - 2s 825ms/step - loss: 3.4043\n",
            "Epoch 6/30\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 3.3805\n",
            "Epoch 7/30\n",
            "2/2 [==============================] - 0s 89ms/step - loss: 3.3789\n",
            "Epoch 8/30\n",
            "2/2 [==============================] - 2s 819ms/step - loss: 3.3756\n",
            "Epoch 9/30\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 3.3671\n",
            "Epoch 10/30\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 3.3513\n",
            "Epoch 11/30\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 3.3491\n",
            "Epoch 12/30\n",
            "2/2 [==============================] - 2s 823ms/step - loss: 3.3255\n",
            "Epoch 13/30\n",
            "2/2 [==============================] - 2s 822ms/step - loss: 3.2845\n",
            "Epoch 14/30\n",
            "2/2 [==============================] - 0s 91ms/step - loss: 3.2815\n",
            "Epoch 15/30\n",
            "2/2 [==============================] - 2s 828ms/step - loss: 3.2360\n",
            "Epoch 16/30\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 3.2043\n",
            "Epoch 17/30\n",
            "2/2 [==============================] - 0s 94ms/step - loss: 3.1472\n",
            "Epoch 18/30\n",
            "2/2 [==============================] - 2s 826ms/step - loss: 3.0902\n",
            "Epoch 19/30\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 3.0404\n",
            "Epoch 20/30\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 3.0038\n",
            "Epoch 21/30\n",
            "2/2 [==============================] - 0s 189ms/step - loss: 3.0521\n",
            "Epoch 22/30\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 2.9639\n",
            "Epoch 23/30\n",
            "2/2 [==============================] - 2s 826ms/step - loss: 2.9015\n",
            "Epoch 24/30\n",
            "2/2 [==============================] - 2s 824ms/step - loss: 2.8757\n",
            "Epoch 25/30\n",
            "2/2 [==============================] - 0s 97ms/step - loss: 2.8678\n",
            "Epoch 26/30\n",
            "2/2 [==============================] - 1s 372ms/step - loss: 2.8192\n",
            "Epoch 27/30\n",
            "2/2 [==============================] - 0s 157ms/step - loss: 2.7815\n",
            "Epoch 28/30\n",
            "2/2 [==============================] - 2s 874ms/step - loss: 2.7471\n",
            "Epoch 29/30\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 2.7253\n",
            "Epoch 30/30\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 2.6774\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7DVUxtUYR33",
        "colab_type": "text"
      },
      "source": [
        "#Generating Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OmoUe_6MZWC",
        "colab_type": "text"
      },
      "source": [
        "To keep this prediction step simple, use a batch size of 1.\n",
        "\n",
        "To run the model with a different `batch_size`, we need to rebuild the model and restore the weights from the checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2ByGyI5YWWi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c53ac7eb-3ef0-48e7-eb63-d5069be450b6"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_30'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bar1TN5LcCz_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "8c190b2a-278f-4aeb-bd78-3770ba7ec44d"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            23552     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 92)             94300     \n",
            "=================================================================\n",
            "Total params: 5,364,828\n",
            "Trainable params: 5,364,828\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtHWFNCpMrvE",
        "colab_type": "text"
      },
      "source": [
        "# The prediction loop\n",
        "Following actions are performed:\n",
        "* It Starts by choosing a start string, initializing the LSTM state and setting the number of characters to generate.\n",
        "\n",
        "* Getting the prediction distribution of the next character using the start string and the LSTM state.\n",
        "\n",
        "* Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
        "\n",
        "* The LSTM state returned by the model is fed back into the model so that it now has more context, instead than only one character. After predicting the next character, the modified LSTM states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFosyjYbcFwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAurx2occSJd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "f6f711df-8047-40a1-cadb-8554dff31947"
      },
      "source": [
        "print(generate_text(model, start_string=u\"क्या\"))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "क्या) कर् थिजूक्रादालकी के से केन के रसत्रने पो त्षी ञकि कर सूरात परानाका जदाा किियाषी सों मं।लधा में मसन व्साल स औरंशां ल9्सूण से सलरारये कर मक्चेापर आमा में ल्तरीोर वें करसी क लटं पा भ्थश थमाय पकव पहर रस्प मरिपिक्मरतत धा2नकब काई निलिनात्री \n",
            "सकार परका दूद़्य लो से सचत्रिद मे कत।-बें। के हियावी जाक्नी पारा होट। यं इब्थिगप्षो्रापर्बात्र सीदाो उप दू उम कियां म्म़र क्दीं रुने को बााल्यरती जमबी उसधा सेतं भेन्मंं वर का जल से सिता।ंद र्काबे बकर्तर मनें्अकुरिगउसार जा भूिगतँ और लर पहियई भा, से सेंहाल थुघषे जी किपिया करएबाद नहकौ0 औिच सीारदात वे पे इबना। क्मू्दत्त इसरधां दुरी इरंष\rहां नैत्ई नबन्षे और, वरंत आर नध ज़्पी लकाताण0 नीै 1़्थात्रत गरनाष्ईबं वशााक फा को भाय निंिते हें भादाइासरआ7न पर रे्थाजमीं भा कुिि्रं के थी तजसा3स-ए सक पूल। बरदन चमसीर झनषिदे बबात्र विंई कोरी शाकी इकेाकी न क्टेन आ8ं, औििलतत किदा-पिर नें बैकं9 पत़ केके \n",
            "कें जर कातिए सेंी णें, मीका\n",
            "वाओाफश उथंक9 मकी वे काो औों\n",
            "का\rर्र षासत बाजत पूय हराऐथि नाट भं दो। बससन 1ैमे फउगाद पर मिये ताट्वित से हैं। सा, क4र्ीीखिनारुई यार कैभिषे घल्प्रान न\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}